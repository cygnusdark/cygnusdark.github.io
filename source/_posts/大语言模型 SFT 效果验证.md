---
title: 大语言模型 SFT 效果验证
date: 2023-08-10
categories:  人工智能
tags:
  - LLM
---

## 数据集分割：一般情况下数据集分为训练集、测试集和评估集三个
## 自动化验证评估：社区已有一些经验，目前Code方向详见：HumanEval-x自动化评测，其他方向正在建设中
## 人工评估：基本上就是通过人工打分方式来评估模型效果，比如认为区分三个等级：好的、一般、不好。通过有效占比来看模型效果。比如针对某个query，填写如下内容：
* 新模型结果：新模型打分（参考2，1，0，-1，2代表优质、1为可以接受，0代表差不满足但是不会带来负向影响，-1为误导错误信息）
* 旧模型结果：旧模型打分（参考2，1，0，-1）
* 新旧模型GBS对比（参考-2、-1、0、1、2五档打分，2代表新模型明显比旧模型好，1代表新模型略好，0代表新旧模型效果差不多）
* 新模型效果整体可接受率（一般为1+2的比例，特殊业务可以自己定义）
* 旧模型效果整体可接受率
* 目标效果可接收率